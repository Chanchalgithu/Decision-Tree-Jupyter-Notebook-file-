{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf077932-a25b-4b6a-95ce-9ca1370c9b9e",
   "metadata": {},
   "source": [
    "Assignment Code: DA-AG-012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412200ac-2606-49fa-bab4-d9b258794d8a",
   "metadata": {},
   "source": [
    "# Decision Tree | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea813b2-e05d-4620-bc57-9e269f52210f",
   "metadata": {},
   "source": [
    "# Question 1: What is a Decision Tree, and how does it work in the context of classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3039365-856c-4df5-a3de-5e4100a8627c",
   "metadata": {},
   "source": [
    "# Answer: \n",
    "\n",
    "A Decision Tree is a popular supervised machine learning algorithm used for both\n",
    "classification and regression problems. In classification tasks, it helps in predicting the\n",
    "class label of input data by learning simple decision rules from the features of the data.\n",
    "\n",
    "It is structured like a tree, where:\n",
    "\n",
    "● The root node represents the entire dataset, which gets split into two or more\n",
    "homogeneous sets based on a specific feature.\n",
    "    \n",
    "● Internal nodes represent decisions based on feature values.\n",
    "    \n",
    "● Branches represent the outcomes of those decisions.\n",
    "    \n",
    "● Leaf nodes represent the final class label or prediction.\n",
    "\n",
    "The decision tree works by recursively partitioning the dataset using algorithms such as\n",
    "ID3, CART, or C4.5, which use metrics like:\n",
    "\n",
    "● Information Gain (based on Entropy),\n",
    "\n",
    "● Gini Index, or\n",
    "\n",
    "● Gain Ratio\n",
    "\n",
    "These metrics measure how well a feature separates the data into target classes.\n",
    "    \n",
    "For example, in a binary classification problem (e.g., \"Yes\" or \"No\"), the decision tree\n",
    "evaluates feature values (like age, income, etc.) step by step, choosing splits that best\n",
    "reduce impurity at each level. This continues until the tree reaches a stopping criterion\n",
    "(e.g., maximum depth, minimum samples per leaf, or perfect classification).\n",
    "\n",
    "Advantages in Classification:\n",
    "\n",
    "● Easy to visualize and interpret\n",
    "\n",
    "● No need for feature scaling\n",
    "         \n",
    "● Handles both numerical and categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f53619-aceb-498b-aeb5-b280a5330281",
   "metadata": {},
   "source": [
    "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a56ba7-2a53-4271-b71c-83b9fd09e952",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "Gini Impurity and Entropy are two commonly used impurity measures in Decision Trees\n",
    "that help determine the best way to split data at each node. Their goal is to increase the\n",
    "\"purity\" of nodes — meaning how homogeneous the classes are after a split.\n",
    "\n",
    "1. Gini Impurity:\n",
    "   \n",
    "● Definition: Gini Impurity calculates the probability of incorrectly classifying a\n",
    "randomly chosen element if it were randomly labeled according to the class\n",
    "distribution.\n",
    "\n",
    "● Formula:\n",
    "\n",
    "Gini = 1 − Σ(pi2)\n",
    "\n",
    "where pi is the probability of class i in the node.\n",
    "\n",
    "● Range: 0 (pure node) to 0.5 (most impure for binary classification).\n",
    "\n",
    "● Properties: Gini tends to isolate the most frequent class and is faster to\n",
    "compute. It is used in algorithms like CART (Classification and Regression\n",
    "Trees).\n",
    "\n",
    "2. Entropy and Information Gain:\n",
    "\n",
    "● Definition: Entropy is a measure from information theory that quantifies the\n",
    "disorder or uncertainty in the dataset. In Decision Trees, we use Information Gain\n",
    "to decide the best split.\n",
    "\n",
    "● Formula for Entropy:\n",
    "\n",
    "Entropy = − Σ(pi × log2(pi))\n",
    "\n",
    "● Information Gain (IG):\n",
    "\n",
    "IG = Entropy(Parent) − Weighted average Entropy(Children)\n",
    "\n",
    "● Range: 0 (pure) to 1 (maximally impure for binary classification).\n",
    "\n",
    "● Properties: Entropy is more theoretical and tends to build more balanced trees,\n",
    "but it is computationally heavier than Gini.\n",
    "\n",
    "3. Impact on Decision Tree Splits:\n",
    "\n",
    "● Both measures are used to evaluate how good a split is.\n",
    "\n",
    "● At each node, the Decision Tree algorithm checks all possible features and\n",
    "thresholds, and chooses the one that results in:\n",
    "\n",
    "  ○ The lowest Gini Impurity, or\n",
    "\n",
    "  ○ The highest Information Gain.\n",
    "\n",
    "● The selected split helps the model create purer child nodes, leading to better\n",
    "classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d83fbf-8fc2-4753-8ca7-c5cbf97957de",
   "metadata": {},
   "source": [
    "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb18737-bbea-4b44-b45d-a536669b300d",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "In Decision Trees, pruning is a technique used to reduce the size of the tree and\n",
    "prevent overfitting. There are two types of pruning: Pre-Pruning and Post-Pruning.\n",
    "\n",
    "# 1. Pre-Pruning (Early Stopping):\n",
    "\n",
    "● Definition: Pre-pruning stops the tree from growing once a certain condition is\n",
    "met, even before it becomes fully grown.\n",
    "\n",
    "● Conditions for Stopping:\n",
    "\n",
    "○ Maximum depth of tree reached\n",
    "\n",
    "○ Minimum number of samples at a node\n",
    "\n",
    "○ Gini or Entropy does not improve significantly\n",
    "\n",
    "● Practical Advantage:\n",
    "\n",
    "➤ Saves time and memory during training by preventing the creation of an\n",
    "overly complex tree.\n",
    "\n",
    "# 2. Post-Pruning (Cost Complexity Pruning):\n",
    "\n",
    "● Definition: Post-pruning allows the tree to grow fully and then removes\n",
    "unnecessary branches that do not contribute much to accuracy.\n",
    "\n",
    "● How it works:\n",
    "\n",
    "○ The complete tree is built\n",
    "\n",
    "○ Cross-validation or a validation set is used to prune the branches that\n",
    "reduce performance\n",
    "\n",
    "● Practical Advantage:\n",
    "\n",
    "➤ Improves model’s ability to generalize by removing overfitted sections of the\n",
    "tree after it’s fully built.\n",
    "\n",
    "# 3. Key Differences between Pre-Pruning and Post-Pruning:\n",
    "\n",
    "● Timing:\n",
    "\n",
    "○ Pre-Pruning: Stops the tree from growing further during training.\n",
    "\n",
    "○ Post-Pruning: Grows the full tree first, then removes unnecessary\n",
    "branches.\n",
    "\n",
    "● Control:\n",
    "\n",
    "○ Pre-Pruning: Applies stopping rules like max depth or minimum samples.\n",
    "\n",
    "○ Post-Pruning: Uses a validation set or cross-validation to decide which\n",
    "branches to prune.\n",
    "\n",
    "● Speed:\n",
    "\n",
    "○ Pre-Pruning: Faster because it builds a smaller tree upfront.\n",
    "\n",
    "○ Post-Pruning: Slower since it grows the entire tree before pruning.\n",
    "\n",
    "● Accuracy:\n",
    "\n",
    "○ Pre-Pruning: Might stop too early and miss useful patterns.\n",
    "\n",
    "○ Post-Pruning: More accurate because it evaluates complete patterns\n",
    "before trimming.\n",
    "\n",
    "● Overfitting Handling:\n",
    "\n",
    "○ Pre-Pruning: Prevents overfitting from the beginning.\n",
    "\n",
    "○ Post-Pruning: Fixes overfitting after it occurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02eca85-9a9f-41b5-aa3b-8c5a9f977d90",
   "metadata": {},
   "source": [
    "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747737c-973b-4c6f-b2a4-156843f59ece",
   "metadata": {},
   "source": [
    "# Answer: \n",
    "\n",
    "Information Gain (IG) is a key concept used in Decision Trees to measure how well a\n",
    "particular feature separates the data into distinct classes. It is based on the concept of\n",
    "Entropy, which measures the impurity or disorder in a dataset.\n",
    "\n",
    "# 1. What is Information Gain\n",
    "\n",
    "● Definition:\n",
    "\n",
    "Information Gain is the reduction in entropy after a dataset is split on a\n",
    "particular feature. It tells us how much \"information\" a feature gives us about the\n",
    "class label.\n",
    "\n",
    "● Formula:\n",
    "\n",
    "Information Gain (IG) = Entropy (Parent) − Weighted Average Entropy (Children)\n",
    "\n",
    "That is:\n",
    "\n",
    "IG = Entropy(parent) − [(n1 / N) × Entropy(child1) + (n2 / N) × Entropy(child2) + ...]\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "○ n1, n2 = number of samples in child nodes\n",
    "\n",
    "○ N = total number of samples in the parent node\n",
    "\n",
    "\n",
    "# 2. Importance in Choosing the Best Split:\n",
    "\n",
    "● At every node in the tree, the algorithm calculates Information Gain for each\n",
    "feature.\n",
    "                                                                      \n",
    "● The feature with the highest Information Gain is selected for the split.\n",
    "                                                           \n",
    "● This leads to the purest possible division of data and helps the tree learn\n",
    "useful patterns.\n",
    "\n",
    "# 3. Simple Example:\n",
    "\n",
    "Suppose we are classifying whether a customer will buy a product.\n",
    "    \n",
    "If the feature Age gives more Information Gain than Income, then the tree will split\n",
    "based on Age — because it provides more useful information to classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f679c3-f80d-40df-a11b-abf521810413",
   "metadata": {},
   "source": [
    "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74651226-6584-492b-a296-5737e601e4a9",
   "metadata": {},
   "source": [
    "# Answer:\n",
    "\n",
    "Decision Trees are widely used in various real-world applications because they are easy\n",
    "to understand, interpret, and implement. Below are some popular applications, along\n",
    "with their advantages and limitations.\n",
    "    \n",
    "# 1. Real-World Applications of Decision Trees:\n",
    "    \n",
    "● Medical Diagnosis:\n",
    "    \n",
    "Used to predict diseases based on symptoms, test results, and patient history.\n",
    "    \n",
    "● Credit Scoring:\n",
    "    \n",
    "Banks use decision trees to approve or reject loan applications based on\n",
    "income, credit score, and other factors.\n",
    "    \n",
    "● Customer Churn Prediction\n",
    "\n",
    "Companies use decision trees to identify customers likely to stop using a\n",
    "service.\n",
    "\n",
    "● Fraud Detection:\n",
    "\n",
    "Helps in identifying unusual transaction patterns that may indicate fraud.\n",
    "    \n",
    "● Marketing and Targeting:\n",
    "    \n",
    "Used to segment customers and personalize advertisements based on behavior.\n",
    "    \n",
    "● Risk Assessment:\n",
    "    \n",
    "Insurance companies use decision trees to evaluate risk and decide premiums.\n",
    "\n",
    "# 2. Main Advantages of Decision Trees:\n",
    "\n",
    "● Easy to Understand and Visualize:\n",
    "    \n",
    "The tree structure is simple to explain and interpret.\n",
    "    \n",
    "● No Need for Data Normalization or Scaling:\n",
    "    \n",
    "Works well with both numerical and categorical data.\n",
    "    \n",
    "● Fast Training Speed:\n",
    "    \n",
    "Especially effective for small to medium-sized datasets.\n",
    "    \n",
    "● Handles Non-Linear Relationships:\n",
    "    \n",
    "Decision Trees can model complex patterns in the data.\n",
    "\n",
    "# 3. Main Limitations of Decision Trees:\n",
    "    \n",
    "● Overfitting:\n",
    "    \n",
    "Trees can become too complex and fit the training data too closely, reducing\n",
    "accuracy on new data.\n",
    "    \n",
    "● Unstable:\n",
    "    \n",
    "Small changes in the data can result in a completely different tree structure.\n",
    "    \n",
    "● Biased with Imbalanced Data:\n",
    "    \n",
    "Decision Trees may favor features with more levels or classes.\n",
    "    \n",
    "● Less Accurate Compared to Ensembles:\n",
    "    \n",
    "Alone, they may perform worse than models like Random Forests or Gradient\n",
    "Boosted Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadb212-e9e1-4351-a7d4-57f99dbe93ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7b4f26-3119-4c98-8bf6-f146ba418d3f",
   "metadata": {},
   "source": [
    "# Dataset Info:\n",
    "\n",
    "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
    "              \n",
    "● Boston Housing Dataset for regression tasks\n",
    "                \n",
    "(sklearn.datasets.load_boston() or provided CSV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57f405-949f-4eb1-bf57-f93231aae293",
   "metadata": {},
   "source": [
    "# Question 6: Write a Python program to:\n",
    "\n",
    "● Load the Iris Dataset\n",
    "\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "\n",
    "● Print the model’s accuracy and feature importances\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f84fbbe-519d-4667-83ed-797447eefae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.4059\n",
      "petal width (cm): 0.5774\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Load the Iris Dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#Step 2: Train a Decision Tree Classifier using Gini\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the classifier\n",
    "df = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "df.fit(X_train, y_train)\n",
    "\n",
    "#Step 3: Print Accuracy and Feature Importances\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = df.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(iris.feature_names, df.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356ebca-d6b9-4fa5-9e54-2758778a5096",
   "metadata": {},
   "source": [
    "# Question 7:\n",
    "\n",
    "Write a Python program to:\n",
    "\n",
    "● Load the Iris Dataset\n",
    "\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a\n",
    "fully-grown tree.\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "857f85e7-d079-4965-8bfb-c8eb49ded6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iris dataset loaded and split successfully!\n",
      "Accuracy (max_depth=3): 1.0000\n",
      "Accuracy (fully-grown): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Load the Iris Dataset \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\" Iris dataset loaded and split successfully!\")\n",
    "\n",
    "# Part 2: Import model and metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Decision Tree with max_depth=3 (shallow tree)\n",
    "df = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "df.fit(X_train, y_train)\n",
    "\n",
    "y_pred_depth3 = df.predict(X_test)\n",
    "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
    "\n",
    "# Fully-grown Decision Tree (no max_depth limit)\n",
    "df = DecisionTreeClassifier(random_state=42)\n",
    "df.fit(X_train, y_train)\n",
    "\n",
    "y_pred_full = df.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "#  Print accuracy comparison\n",
    "print(f\"Accuracy (max_depth=3): {accuracy_depth3:.4f}\")\n",
    "print(f\"Accuracy (fully-grown): {accuracy_full:.4f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3d5d5-6082-4488-bc13-981692ccac1e",
   "metadata": {},
   "source": [
    "# Question 8:\n",
    "\n",
    "Write a Python program to:\n",
    "\n",
    "● Load the Boston Housing Dataset\n",
    "\n",
    "● Train a Decision Tree Regressor\n",
    "\n",
    "● Print the Mean Squared Error (MSE) and feature \n",
    "\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea29d6fd-29aa-4ed1-ba41-b56ddf670a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5280096503174904\n",
      "Feature Importances:\n",
      "MedInc: 0.5235\n",
      "HouseAge: 0.0521\n",
      "AveRooms: 0.0494\n",
      "AveBedrms: 0.0250\n",
      "Population: 0.0322\n",
      "AveOccup: 0.1390\n",
      "Latitude: 0.0900\n",
      "Longitude: 0.0888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Regressor\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(california.feature_names, model.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d4d79-9c39-429f-abde-805295ba14e2",
   "metadata": {},
   "source": [
    "# Question 9: \n",
    "\n",
    "Write a Python program to:\n",
    "\n",
    "● Load the Iris Dataset\n",
    "\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
    "\n",
    "● Print the best parameters and the resulting model accuracy\n",
    "\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09a32034-378c-4776-b759-34623e55f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
      "Model Accuracy on Test Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions and evaluate accuracy\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Model Accuracy on Test Set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58515b2-66d6-4e07-b56a-e184b1ec6748",
   "metadata": {},
   "source": [
    "# Question 10:\n",
    "\n",
    "Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "    \n",
    "Explain the step-by-step process you would follow to:\n",
    "\n",
    "● Handle the missing values\n",
    "\n",
    "● Encode the categorical features\n",
    "\n",
    "● Train a Decision Tree model\n",
    "\n",
    "● Tune its hyperparameters\n",
    "\n",
    "● Evaluate its performance And describe what business value this model could provide\n",
    "in the real-world setting.\n",
    "\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84202fea-1ad5-4dd6-881f-7f24485ad3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'classifier__max_depth': 2, 'classifier__min_samples_split': 2}\n",
      " Accuracy on Test Set: 1.0\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For numerical columns\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# For categorical columns\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Encoder for categorical data\n",
    "categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [25, 52, 47, np.nan, 33, 61, np.nan],\n",
    "    'gender': ['Male', 'Female', 'Female', 'Male', np.nan, 'Male', 'Female'],\n",
    "    'bmi': [22.4, 28.5, np.nan, 25.3, 30.2, np.nan, 26.1],\n",
    "    'smoker': ['No', 'Yes', 'No', 'Yes', 'No', 'Yes', np.nan],\n",
    "    'has_disease': [0, 1, 0, 1, 0, 1, 1]\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = data.drop('has_disease', axis=1)\n",
    "y = data['has_disease']\n",
    "\n",
    "# Column types\n",
    "numeric_cols = ['age', 'bmi']\n",
    "categorical_cols = ['gender', 'smoker']\n",
    "\n",
    "# Preprocessing for numeric and categorical\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine all in column transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_cols),('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Final pipeline with Decision Tree\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Grid for tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [2, 3, 4],\n",
    "    'classifier__min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=2)\n",
    "\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure GridSearchCV is already fitted\n",
    "# best_model = grid_search.best_estimator_   # Already retrieved from GridSearchCV\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\" Best Parameters:\", grid_search.best_params_)\n",
    "print(\" Accuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
    "print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173a34b-ecd7-49c6-978f-ae22bc20b48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
